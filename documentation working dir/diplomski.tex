% Predlozak za pisanje diplomskog rada na PMF-MO
% Opcenita uputstva za LaTeX se mogu npr. naci na 
% http://web.math.hr/nastava/rp3, http://web.math.hr/nastava/s4-prof/latex.pdf
% NE PREPORUCA se "Ne baš tako kratak uvod u TEX", buduci se radi o vrlo starom prirucniku
% koji nije pogodan za moderne verzije LaTEXa.
% Originalna verzija "The not so short..." na http://tobi.oetiker.ch/lshort/lshort.pdf 
% je obnovljena i daje bolji uvid u moderne verzije LaTeXa

% Stil je optimiziran za kreiranje pdf dokumenta (npr. pomocu pdflatex-a, XeLaTeX-a)

\documentclass[a4paper,twoside,12pt]{memoir} % jednostrano: promijeniti twoside u oneside

% Paket inputenc omogucava direktno unosenje hrvatskih dijakritickih znakova 
% opcija utf8 za unicode (unix, linux, mac)
% opcija cp1250 za windowse
\usepackage[utf8]{inputenc}  % ukoliko se koristi XeLaTeX onda je \usepackage{xunicode}\usepackage{xltxtra}

% Stil za diplomski, unutra je ukljucena podrska za hrvatski jezik
\usepackage{diplomski}
% bibliografija na hrvatskom
\usepackage[languagenames,fixlanguage,croatian]{babelbib} % zahtijeva datoteku croatian.bdf
% hiperlinkovi 
\usepackage[pdftex]{hyperref} % ukoliko se koristi XeLaTeX onda je \usepackage[xetex]{hyperref}

% Odabir familije fontova:
% koristenjem XeLaTeX-a mogu se koristiti svi fontovi instalirani na racunalu, npr
% \defaultfontfeatures{Mapping=tex-text}
% \setmainfont[Ligatures={Common}]{Hoefler Text}
% ili
% \newcommand{\nas}[1]{\fontspec{Adobe Garamond Pro}\fontsize{24pt}{24pt}\color{Chocolate}\selectfont #1}
% i onda \nas{Naslov ...}
\usepackage{txfonts} % times new roman 

% Paket graphicx sluzi za manipuliranje grafikom 
\usepackage[pdftex]{graphicx} % ukoliko se koristi XeLaTeX onda je \usepackage[xetex]{graphicx}
% Paket amsmath je vec ukljucen
% Dodatno definirane matematicke okoline:
% teorem (okolina: thm), lema (okolina: lem), korolar (okolina: cor),
% propozicija (okolina: prop), definicija (okolina: defn), napomena (okolina: rem),
% slutnja (okolina: conj), primjer (okolina: exa), dokaz (okolina: proof)
% Definirane su naredbe za ispisivanje skupova N, Z, Q, R i C
% Definirane su naredbe za funkcije koje se u hrvatskoj notaciji oznacavaju drukcije 
% nego u americkoj: tg, ctg, ... (\tgh za tangens hiperbolni)
% Takodjer su definirane naredbe za Ker i Im (da bi se razlikovala od naredbe za imaginarni dio kompleksnog
% broja, naredba se zove \slika).

\pagestyle{headings}
% uz paket fancyhdr mogu se lako kreirati fancy zaglavlja i podnozja

% Podaci koje treba unijeti
\title{Paralelni algoritmi za problem grupiranja podataka}
\author{Anto Čabraja}
\advisor{prof. dr. sc. Goranka Nogo}  % obavezno s titulom (prof. dr. sc ili doc. dr. sc.)
\date{srpanj 2014.}  % oblika mjesec, godina

% Moguce je unijeti i posvetu
% Ukoliko nema posvete, dovoljno je iskomentirati/izbrisati sljedeci redak 
%\dedication{Samom sebi}

\begin{document}

% Naredna frontmatter generira naslovnu stranicu, stranicu za potpise povjerenstva, eventualnu posvetu i sadrzaj
% Moze se iskomentirati ukoliko nije u pitanju konacna verzija
\frontmatter

% Tekst diplomskog ...

% Diplomski rad treba poceti s uvodnim poglavljem  
\chapter*{Uvod}
\section[Problem grupiranja podataka][PGP]{Problem grupiranja podataka}
\section[Primjena][primjena]{Primjena}
\section[Pregled rada][pregled]{Pregled rada}

\chapter[Modeliranje problema grupiranja][MODELIRANJE]{Modeliranje problema grupiranja}	
% ukoliko naslov nije jako dugacak dovoljno je samo \chapter{Naslov poglavlja} 

\section[Osnovni pojmovi][os-pojmovi]{Osnovni pojmovi}
\label{sec:os-pojmovi}
Kako bi u daljnjem razmatranju bilo jednostavnije objašnjavati strukture i same implementacije algoritama potrebno je problem grupiranja reprezentirati osnovnim pojmovima. U nastavku ćemo formalno definirati sve komponente od kojih se problem grupiranja sastoji.
\begin{defn}
\label{def:uzorak}
\textbf{Uzorak} je apstraktna struktura podataka koja reprezentira stvarne podatke s kojima raspolaže algoritam za grupiranje.
\end{defn}
\begin{defn}
\label{def:svojstvo}
\textbf{Svojstvo} je vrijednost ili struktura koja predstavlja jednu značajku danog podatka unutar uzorka.
\end{defn}
\begin{defn}
\label{def:udaljenost}
\textbf{Udaljenost} između uzoraka definiramo kao funkciju 
$f: D  -> \R$, gdje je $D$ skup svojstava danih uzoraka
\end{defn}
\begin{defn}
\label{def:blizina}
Za uzorke kažemo da su \textbf{blizu} jedan drugome ako je njihova udaljenost manja od unaprijed zadane veličine
\end{defn}
\begin{defn}
\label{def:klaster}
\textbf{Klaster} je skup uzoraka koji su u prostoru podataka blizu. Ako su uzorci identični onda je njihova udaljenost uvijek $0$
\end{defn}
\begin{defn}
\label{def:hard}
\textbf{Jednistveno grupiranje} je postupak grupiranja kada svaki uzorak pripada jednom i samo jednom klasteru.
\end{defn}
\begin{defn}
\label{def:fuzzy}
\textbf{Nejasno ili nejedinstveno grupiranje} je postupak grupiranja gdje jedan uzorak može biti u više klastera.
\end{defn}
\begin{rem}
U radu ćemo promatrati \textbf{jedinstveno grupiranje} tako da će sve daljnje definicje i modeliranja predpostavljati da želimo dobiti disjunktne klastere.
\end{rem}

\section[Matematičko modeliranje problema][Matematičko modeliranje]{Matematičko modeliranje problema}
Definicija grupiranja podataka nije jedinstvena. U literati se na različite načine pokušava opisati ovaj postupak. Neki od pokušaja opisne definicije su:

\begin{enumerate}
\item \textit{Grupiranje podataka je postupak otkrivanja homogenih\footnote{podaci koji se ne mogu smisleno separirati} grupa uzoraka unutar skupa svih danih uzoraka.}

\item \textit{Grupiranje podataka je postupak određivanja koji su uzorci slični te ih svrstati u isti klaster.}
\end{enumerate}
Za modelirali problem neće nam biti dovoljne opisne definicije. U ovom slučaju opisne definicje mogu poslužiti samo kao intuicija o ćemu se zapravo radi kada govorimo o grupiranju. U nastavku ćemo pomoću definiranih pojmova u poglavlju \ref{sec:os-pojmovi} matematički opisati problem grupiranja podataka.

Neka je $\mathbf{U} = \{U_1,U_2,\cdots,U_n\} $ skup od $n$ uzoraka,te neka je $U_i = (s_1,s_2,\cdots,s_d)$ reprezentiran $d$-dimenzionalnim vektorom gdje $s_i$ predstavlja jedno svojstvo. Ovako definiran $\mathbf{U}$ moguće je reprezentirati kao matricu $\mathbf{S}_{d\times n}$.Svaki stupac te matrice predstavlja jedan uzorak iz danog skupa $\mathbf{U}$.
\begin{equation}
\label{equ:matrica-uzoraka}
\mathbf{S}_{d \times n} = 
\begin{pmatrix}
s_{1,1} & s_{1,2} & \cdots & \cdots & s_{1,n}\\
s_{2,1} & s_{2,2} & \cdots & \cdots & s_{2,n}\\
\vdots & \vdots & \ddots & \ddots & \vdots\\
s_{d,1} & s_{d,2} & \cdots & \cdots & s_{d,n}
\end{pmatrix}
\end{equation}
Iz definicje \ref{def:hard} te iz navedenog formalnog zapisa dajemo formalnu definiciju problema grupiranja.
\begin{defn}
\label{def:k-klaster}
\textbf{Skup od k klastera} $\mathbf{C} = \{C_1,C_2,...,C_k\}$ je skup sa sljedećim svojstvima:
\begin{itemize}
\item $C_i \neq \emptyset$
\item $C_i \cap C_j = \emptyset$ , $\forall i,j$ $ i \neq j$
\item $\cup_{i=1}^{k}C_i = \mathbf{U} $
\end{itemize} 
\end{defn}
\begin{rem}
\label{nap:ZC}
U terminima matrice $\mathbf{S}$ to znači da se svaki $C_i$ zapravo sastoji od stupaca matrice $\mathbf{S}$.
\end{rem}
\begin{defn}
\label{def:hard-formal}
\textbf{Problem grupiranja} u skup od k klastera $\mathbf{C}$ je ekvivalentan problemu da\\
$\forall c, c' \in C_i$ udaljenost od $c$ do $c'$ je manja od udaljenosti $c$ do bilo kojeg drugog $c'' \in C_j$ $ j \neq i$ 
\end{defn}
Zapravo problem grupiranja je pronalazak najpogodnije particije za $\mathbf{C}$ u skupu svih mogućih particija.
Prema napomeni \ref{nap:ZC} lako se zaključi da se problem grupiranja svodi na problem raspodjele n stupaca matrice $\mathbf{S}$ u $k$ skupova.
Uvedena matematička notacija za problem grupiranja omogućuje da postavimo model za rješavanje, koji je u kasnijem razmatranju pogodan za modeliranje i implementaciju.\\
Neka je  $\mathcal{C} = \{ \mathbf{C}^1, \mathbf{C}^2, ..., \mathbf{C}^{N(n,k)}  \}$ skup svih mogućih rješenja danog problema grupiranja, gdje je
\begin{equation}
N(n,k) = \frac{1}{k!}\sum_{i=1}^{k}(-1)^i{{k}\choose{i}}(k-i)^i
\end{equation}
broj mogućih rješenja za raspodjelu $n$ uzoraka u $k$ klastera. Rješenje problema svodi se na optimizacijski problem
\begin{equation}
\label{equ:opt}
\underset{\mathbf{C} \in \mathcal{C}}{optimiziraj} \textbf{  }f(\mathbf{S}_{d\times n}, \mathbf{C})
\end{equation}
gdje je $f$ funkcija dobrote rješenjea $\mathbf{C}$, Funkcija dobrote je funkcija koja mjeri kvalitetu rješenja za dani problem. Ovisno o tome kako je zadana može se gledati problem maksimizacije ili minimizacije. Gotovo uvijek njome se određuju jedan od dva važna parametra:
\begin{itemize}
\item koliko su blizu uzorci u danom klasteru
\item koliko su blizu dva disjunktna klastera
\end{itemize}
Ako promatramo udaljenosti uzoraka unutar klastera, onda nam se problem optimizacije \ref*{equ:opt} svodi na problem minimizacije funkcije $f$, jer želimo postići što manju udaljenost uzoraka unutar jednog klastera. Međutim ako želimo postići da su nam klasteri međusobno disjunktni i da granica disjunkcije bude čvrsto definirana moramo promatrati udaljenosti među klasterima. U ovom slučaju potrebno je maksimizirati problem to jest tražiti za koju particju će vrijednost funkcije $f$ biti najveća. \\
Konačno, sada znamo uzorke reprezentirati kao $n$-dimenzionalne vektore, također znamo definirati klaster kao skup vektora, te smo postavili model za određivanje kvalitete određenog klastera. Preostali posao je pronaći konkretnu funkciju $f$ koja će na adekvatan način reprezentirati udaljenost između uzoraka.Vrlo je važno definirari od koji se komponenti uzorak sastoji. Osnovna podjela uzoraka je na \textit{numeričke} i \textit{kategoričke}. Numerički uzorak je onaj uzorak čije su sve vrijednosti numeričkog tipa, dok je kategorički onaj uzorak čije vrijednosti poprimaju vrijednosti nekih kategorija. Više o mogućnostima izgleda svojstava uzorka biti će rečeno u cijelini \ref{sec:upravljanje-pod}. U ovom trenutku za definiranje funkcije dobrote potrebno je samo imati u vidu da podaci ne moraju biti jednostavni, ali i dalje se od funkcije dobrote očekuje da na adekvatan način odredi udaljenost između dva uzorka.
\subsubsection{Uzorak s numeričkim značajkama}
Ukoliko su nam uzorci takvi da ih možemo predstaviti kao vektor numeričkih podataka tada ih možemo smjestiti u vektoriski prostor, te na njima upotrijebiti neku od standardnih vektorskih normi. Za problem grupiranja podataka u praksi najčešće se koristi Euklidska ili neka od p-normi [liter]. 
Euklidska norma daleko je najpopularnija i glavni je predstavnik normi koje mjere različitost među uzorcima. Drugim rječima za Euklidsku normu vrijedi da su uzorci više različiti što je vrijednost norme veća. \\
Neka su $S_1$ i $S_2$ dva uzorka sa $n$ značajki, tada udaljenost $d$ između dva uzorka u eklidskoj normi računamo kao:
\begin{equation}
\label{equ:metrika-num}
d(S_1,S_2) = \sqrt{\sum_{i=1}^{n}(s_{i,1} -s_{i,2})^2}
\end{equation}
gdje su $s_{i,1}$ i $s_{i,2}$ značajke u uzorcima $S_1$ i $S_2$. U kreiranju rješenja sa numeričkim podacima uvijek ćemo koristit Euklidsku normu s malim izmjenama ovisno o vrsti problema. Međutim u praksi se vrlo četo pojavljuje i noram koja koristi svojstva kovarijacijske matrice uzoraka[liter]. 
\begin{equation}
d(S_1,S_2) = (S_1-S_2)^T\Sigma^{-1}(S_1-S_2)
\end{equation}
$S_1$ i $S_2$ uzorci, $\Sigma $ kovarijacijska matrica uzoraka. I za ovo normu također vrijedi da su podaci više različiti što je vrijednost $d$ veća. Također postoji veza između Euklidske i norme s kovarijacijskom matricom što je detaljno objašnjeno u [liter]. Samo ćemo reći da ako je $\Sigma$ dijagonalna matrica onda se pripadna udaljenost zove normalizirana Euklidska udaljenost.\\
Osim normi koje mjere razlike među podacima, postoje i norme koje mjere sličnot. Norme koje mjere sličnost često su baziranje na određivanju kuta između dva uzorka u vektorskom prostoru. U [liter] objašnjene su neke od popularnijih normi ovog tipa.
\subsubsection{Uzorak s kategoričkim značajkama}
Individualna usporedba  dva uzorka s kategoričkim značajkama vrlo često nema važnost i jedina povratna informacija je koliko značajki u uzorcima ima jednaku vrijednost. Ako ipak želimo postići da usporedbom dvije kategorije možemo zaključiti koliko su značajke u uzorku slične, to jest blizu, moramo svakoj značajki pridružiti težine i definirati operaciju razlike. Ako bolje pogledamo tim postupkom zapravo smo kategorije pretvorili u numeričke vrijednosti sa posebno definiranom funkcijom razlike.
Promotrimo sljdeći primjer.
\begin{exa}
Neka nam je na raspolaganju skup podataka o cvijeću, te neka nam je svaki cvijet zadan kao vektor s tri kategoričke komponente. Ounačimo sa $S$ i $S'$
dva cvijeta iz skupa zadanih uzoraka.
\begin{align*}
 S &= (crven, amerika, iglicasto)\\
 S'& = (plav, europa, iglicasto)
\end{align*}
Lako vidimo da jedini zaključak kojeg iz ovih podataka možemo dobiti jest da se na prve dvije komponente razlikuju, dok je na trećoj vrijednost kategorije ista. Ako bi naprimjer htjeli kategorijama pridružiti vrijednost morali bi definirati što znači da su amerika i europa različite i koliko nam je to bitno svojstvo. Važnost svojstva ima ključnu ulogu i vrlo je teško empirijski procijeniti kakve numeričke podatke dodjeliti kategorijama.
\end{exa} 
U praksi nismo uvijek u mogućnosti predvidjeti sve moguće kategoričke vrijednosti, pa samim time ne možemo svakoj kategoriji pridružiti numeričku vrijednost. Često korištena i popularna metoda za određivanje pripadnosti određenoj klasi, kada se radi o kategoričkim vrijednostima, je traženje udjela pojavljivanja određene kategoričke vrijednosti u promatranom klasteru.U tom slučaju ne promatramo udaljenost dva uzorka nego koliko u klasteru ima kategorički vrijednosti sličnih onima koje obilježavaju novi uzorak.
Drugim rječima promatramo sličnost uzorka sa klasterom.
Neka je $\mathbf{C} = \{C_1,C_2,...C_n\}$ skup od $n$ klastera. Neka je $c = (c_1,c_2,...,c_d)$ uzorak sa $d$ značajki istog tipa kao i uzorci iz zadanog skupa klastera. Definiramo funkciju sličnosti $s$ kao
\begin{equation}
\label{equ:omjer_znacajki}
s(c,C_x) =  \sum_{i=1}^{d}\frac{ find(c_i,C_x)}{|C_x|}
\end{equation} 
gdje je $C_x \in \mathbf{C}$, funkcija $find$ vraća broj pojavljivanje značajke $c_i$ na $i$-tom mjestu u svakom uzorku iz skupu $C_x$, a $|C_x|$ označava broj uzoraka u skupu $C_x$. Važno je naglasiti da se promatra samo $i$-to mjesto u svim značajkama. Postoji mogućnost da se vrijednost značajke, to jest kategorija, na $i$-tom i $j$-tom mjestu u uzorku podudaraju. Na primjer ako imamo uzorak koji predstavlja automobile, te su dvije od značajki danog uzorka boja interijera i boja automobila. Očito dvije navedene značajke mogu poprimiti istu vrijednost, ali zapravo su potpuno disjunktne i kao takve ih treba promatrati. Konačno $c$ pridružimo onom klasteru za koji funkcija $s$ vrati najveću vrijednost.\\ Kada promatramo određeni skup podataka i kreiramo vektore uzoraka, u svakom trenutku znamo veličinu tog uzorka. Prilikom modeliranja problema sami određujemo skup značajki koji koristimo. Saznanje o broju značajki i njihovoj važnosti možemo upotrijebiti kako bi svakoj značajki odredili težinu. Naime, kako nisu sve značajke jednako bitne želimo postići da se utjecaj nekih značajki restringira, odnosno naglasi.
S obzirom na navedeni zahtjev modificiramo funkciju $s$ iz \ref{equ:omjer_znacajki} te definiramo novu funkciju $s_w$.
\begin{equation}
\label{equ:omjer_znac_tezine}
s_w(c,C_x) = \sum_{i=0}^{d}w_i\frac{find(c_i,C_x)}{|C_x|}
\end{equation}
Kao što vidimo svakoj značajki unutar uzorka pridružili smo težinu $w_i$. Važno svojstvo funkcije $s_w$ je da omogućuje potpuno isključivanje nekih od značajki. Navedeno svojstvo u praksi često služi za empirijsko otkivanje nebitnih značajki. Otkrivanje i uklanjanje značajki čije postojanje ne pridonosi poboljšanju rješenja uvelike smanjuje prostornu složenost što će detaljno biti obrađeno u \ref{sec:upravljanje-pod}.
\subsubsection{Uzorak s hibridnim značajkama}
Priroda problema za koje pokušavamo rješiti problem grupiranja često je složene strukture te je nemoguće podatke reprezentirati uzorcima za značajkama samo jednog tipa. U takvom slučaju podatak se reprezentira za uzorkom čije značajke mogu biti numeričkog i kategoričkog tipa. Ovako definirana reprezentacija podataka povećava složenost i modeliranje funkcije dobrote, odnosno odluka o tome koliko su dva uzorka slična.
Označimo sa $S = (s_1,s_2,...,s_n)$ uzorak od $n$ značajki numeričkog i kategoričkog tipa. Bez smanjenja općenitosti možemo predpostaviti da su na prvih $l$ komponenti numeričke značajke, a na ostalih $n-l$ kategoričke. Funkciju udaljenosti $d_h$ između dva uzorka $S_1 $ i $S_2$ s hibridnim značajkama definiramo kao
\begin{equation}
d_h(S_1, S_2) = d(S_1(:l),S_2(:l)) + \frac{1}{s_w(S_1(l+1:),C_{S_2})}
\end{equation}
gdje je $d$ funkcija definirana u \ref{equ:metrika-num}, $s_w$ definirana u \ref{equ:omjer_znac_tezine}, a $C_{S_2}$ označava klaster u kojemu se nalazi $S_2$. Važno je napomenuti da $S_1$ ne mora nužno biti u klasteru dok $S_2$ mora. Dakle ako dodajemo novi uzorak i želimo mu pronaći klaster to ćemo učiniti tako da ga stavimo kao prvi argument funkcije $d_h$. Kao što vidimo vrijednost funkcije $s_w$ može biti jednaka nuli pa nam drugi sumand u definiciji funkcije $d_h$ nije definiran. U praksi se eksperimentalno odredi kolika je važnost da uzorak nema niti jednu katekoričku značajku istu kao značajke unutar promatranog klastera, te se funkcija $d_h$ dodefinira s obzirom na slučaj kada je $s_w = 0$. Neka je $\lambda$ eksperimentalno određena vrijednost tada se potpunda definicija funkcije $d_h$ može zapisati 
\begin{equation}
d_h(S_1,S_2) = \left\{ 
  \begin{array}{l l}
    d(S_1(:l),S_2(:l)) + \frac{1}{s_w(S_1(l+1:),C_{S_2})} & \quad s_w \neq 0\\
    \\
    d(S_1(:l),S_2(:l)) + \lambda & \quad s_w=0
  \end{array} \right.
\end{equation}
Ovako definitrana funkcija $d_h$ je korektna. Naime, za sve parove uzoraka uvijek možemo odrediti vrijednost funkcije $d_h$ jer su sve komponente dane funkcije dobo definirane. Funkcija $d$ je zapravo standardna Euklidska norma pa je kao takva definirana za sve vektore. Funkcija $s_w$ je kompozicija funkcije $find$, funkcije zbrajanja, množenja i kardinalnog broja. Sve funkcije su dobro definirane[liter] pa je i kompozicija dobro definiran. Primjerimo da se ovako definirana funkcija $d_h$, sa pravilno postavljenim paremetrom $\lambda$ može koristit za sve slučajeve uzoraka: numeričke, kategoričke ili hibridne.
\section[Metode razvoja algoritama za grupiranje][metode razvoja]{Metode razvoja algoritama za grupiranje}
U postupku modeliranja rješenja za problem grupiranja koristi se više metoda. Metoda za rješavanje ima puno i znastvenici intenzivno rade na pronalasku novih i objašnjenju kvalitete postojećih metoda. Danas je ovo područje izuzetno cjenjeno i svako novo saznanje može dovesti do revolucionarnog napretka u rješavanju izuzetno teških problema. Ipak, dvije metode su se pokazale kao općenitije i mogu poslužiti kao predložak za rješavanje većeg broja problema. Konkretno radi se o \textit{hijerarhijskom} i \textit{particijskom} pristupu rješavanju problema. Među ostalim metodama koje se danas ističu kao jako korisne pokazale su se : evolucijske metode, metode particioniranja grafa, metode bazirane na neuronskim mrežama, te metode s gradinjentnim spustom[liter]. U ovom radu obradit ćemo navedene najpoznatije metode, a od ostalih metoda obradit ćemo evolucijske. Također pokazat ćemo kako se komponente iz evolucijskih metoda mogu kombinirati sa particijskim pristupom, te će ta hibridna kombinacija biti glavna tema u nastavku ovog rada.

\subsubsection{Hijerarijsko grupiranje}

\section[Upravljanje podacima][upravljanje-podacima]{Upravljanje podacima}
\label{sec:upravljanje-pod}
\chapter[Metaheuristike]{Meta-heuristički pristup problemu}
\section[Prirodom inspirirani algoritmi][prirodni-algoritmi]{Prirodom inspirirani algoritmi}
\section{Reprezentacija podataka}
\section{Analiza rezultata}
\chapter{Poznati algoritmi i analiza}
\section{Alg 1}
\section{Alg 2}
\section{Alg 3}
\chapter{Tehnike za paralelizaciju algoritama}
\section[Osnovni pojmovi MPI tehnologije][mpi]{Osnovni pojmovi MPI tehnologije}
\section[Topologija][topologija]{Topologije}
\section[Prednosti paralelizacije i cijena komunikacije][pred-man-paralel]{Prednosti paralelizacije i cijena komunikacije}
\chapter{Konstrukcija paralelnih algoritama za grupiranje}
\section{Algoritam 1 heurisika}
\subsection{Opis}
\subsection{Analiza}
\section{Algoritam 2 iterativno}
\subsection{Opis}
\subsection{Analiza}
\section{Algoritam 3 hibrid}
\subsection{Opis}
\subsection{Analiza}
\chapter{Ostale moderne metode}
\section{Programiranje na grafičkim karticama}
\section{MapReduce metoda}
% Na kraju diplomkog rada stavlja se  bibliografija
% Najprije definiramo nacin prikazivanja bibliografije, u ovom slucaju verzija amsplain stila
\bibliographystyle{babamspl} % babamspl ili babplain

% U datoteku diplomski.bib se stavljaju bibliografske reference
% Bibliografske reference u bib formatu se mogu dobiti iz MathSciNet baze, Google Scholara, ArXiva, ...
\bibliography{diplomski}

\pagestyle{empty} % ne zelimo brojanje sljedecih stranica

% I na koncu idu sazeci na hrvatskom i engleskom

\begin{sazetak}
Ukratko ...
\end{sazetak}

\begin{summary}
In this ...
\end{summary}

% te zivotopis

\begin{cv}
Na slici \ref{def:fuzzy} se nalazi 3D graf neke funkcije. 

\begin{figure}[h!t]
\centering \includegraphics{surface3d.png}
\caption{Druga slika}
\label{fig:3d}
\end{figure}

kao i jedna vrlo komplicirana formula koja slijedi iz \eqref{eq:jed1}
\[ \sum_{i=1}^{\infty}A_{x_1}\times A_{{\alpha}_2}\oslash\iint_{\Omega}x^2\ddagger\limsup_{n\in\N}\frac{\alpha+\theta+\gamma}{n^{\omega}}\;\;\text{je u stvari}\;\;\biguplus_{r\in\Q}\overline{\Xi_i \mathop\Theta_{\substack{j\in\C \\ j\ni i\Q}} \Upsilon^{k^j} \underset{\ast}{\Psi} \hslash\vert_{\{\alpha\}}}.\]
\end{cv}

\end{document}